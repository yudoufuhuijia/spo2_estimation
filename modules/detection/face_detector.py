import cv2
import numpy as np
import time
from typing import List, Tuple, Optional, Dict
import warnings
import os

# ä¼˜å…ˆå…³é—­TensorFlow oneDNNå†—ä½™æ—¥å¿—ï¼ˆå¿…é¡»åœ¨å¯¼å…¥MTCNNå‰æ‰§è¡Œï¼‰
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

# å°è¯•å¯¼å…¥MTCNNï¼ˆæ ¹æ®è¯Šæ–­ç»“æœé€‚é…ï¼‰
try:
    from mtcnn import MTCNN

    HAS_MTCNN = True
except ImportError:
    print("âš ï¸  MTCNNæœªå®‰è£…ï¼Œå°†ä½¿ç”¨OpenCV Haar Cascadeå¤‡é€‰æ–¹æ¡ˆ")
    HAS_MTCNN = False

# å¿½ç•¥æ— å…³è­¦å‘Š
warnings.filterwarnings('ignore')


class FaceDetector:
    """
    è½»é‡åŒ–äººè„¸æ£€æµ‹å™¨ï¼ˆå®Œå…¨é€‚é…MTCNNè¯Šæ–­ç»“æœï¼‰
    æ ¸å¿ƒé€‚é…ï¼šMTCNNä»…æ”¯æŒstages/deviceå‚æ•°ï¼Œæ— min_face_size/scale_factor
    """

    def __init__(
            self,
            method: str = 'mtcnn',
            min_face_size: int = 40,  # ä»…ç”¨äºæ£€æµ‹åè¿‡æ»¤ï¼Œä¸ä¼ å…¥MTCNN
            confidence_threshold: float = 0.9,
            mtcnn_device: str = 'CPU:0'  # MTCNNæ”¯æŒçš„deviceå‚æ•°ï¼ˆè¯Šæ–­ç»“æœç¡®è®¤ï¼‰
    ):
        """
        åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨ï¼ˆå‚æ•°é€‚é…è¯Šæ–­ç‰ˆMTCNNï¼‰

        Args:
            method: æ£€æµ‹æ–¹æ³• ('mtcnn'ä¼˜å…ˆï¼Œ'haar'å¤‡é€‰)
            min_face_size: æœ€å°äººè„¸å°ºå¯¸ï¼ˆæ£€æµ‹åè¿‡æ»¤ï¼Œåƒç´ ï¼‰
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆMTCNNç»“æœè¿‡æ»¤ï¼‰
            mtcnn_device: MTCNNè®¾å¤‡ï¼ˆè¯Šæ–­æ”¯æŒ'CPU:0'ï¼Œæ— éœ€ä¿®æ”¹ï¼‰
        """
        self.method = method
        self.min_face_size = min_face_size  # ä»…ç”¨äºåè¿‡æ»¤
        self.confidence_threshold = confidence_threshold
        self.mtcnn_device = mtcnn_device

        self.detector = None
        self.detection_count = 0  # æ€»æ£€æµ‹æ¬¡æ•°
        self.total_time = 0.0  # æ€»æ£€æµ‹è€—æ—¶ï¼ˆç§’ï¼‰

        # åˆå§‹åŒ–æ£€æµ‹å™¨ï¼ˆè‡ªåŠ¨å¤„ç†å‚æ•°é€‚é…ï¼‰
        self._init_detector()

    def _init_detector(self):
        """åˆå§‹åŒ–æ£€æµ‹å™¨ï¼ˆæ ¹æ®æ–¹æ³•è‡ªåŠ¨é€‚é…ï¼Œå¤±è´¥é™çº§ï¼‰"""
        print(f"ğŸ”§ åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨ (æ–¹æ³•: {self.method})...")

        if self.method == 'mtcnn' and HAS_MTCNN:
            self._init_mtcnn_detector()  # é€‚é…è¯Šæ–­ç‰ˆMTCNN
        else:
            # MTCNNä¸å¯ç”¨ï¼Œå¼ºåˆ¶åˆ‡æ¢åˆ°Haar
            print(f"âš ï¸  MTCNNä¸å¯ç”¨ï¼ˆæœªå®‰è£…/åˆå§‹åŒ–å¤±è´¥ï¼‰ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°Haar Cascade")
            self.method = 'haar'
            self._init_haar_detector()

    def _init_mtcnn_detector(self):
        """åˆå§‹åŒ–MTCNNï¼ˆä¸¥æ ¼æŒ‰è¯Šæ–­ç»“æœä¼ å‚ï¼šä»…stages/deviceï¼‰"""
        try:
            # è¯Šæ–­ç¡®è®¤ï¼šMTCNN.__init__ä»…æ”¯æŒstageså’Œdeviceå‚æ•°
            self.detector = MTCNN(
                stages='face_and_landmarks_detection',  # é»˜è®¤å€¼ï¼Œä¿ç•™æ˜¾å¼ä¼ å‚
                device=self.mtcnn_device  # è¯Šæ–­æ”¯æŒçš„è®¾å¤‡å‚æ•°
            )
            print(f"âœ… MTCNNæ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
            print(f"   è®¾å¤‡: {self.mtcnn_device}")
            print(f"   å…³é”®ç‚¹æ”¯æŒ: âœ…ï¼ˆè‡ªåŠ¨è¿”å›åŒçœ¼/é¼»å­/åŒå˜´è§’ï¼‰")

        except TypeError as e:
            # æç«¯æƒ…å†µï¼šå‚æ•°ä»ä¸å…¼å®¹ï¼Œå°è¯•æ— å‚æ•°åˆå§‹åŒ–ï¼ˆè¯Šæ–­æ¨èæœ€å®‰å…¨æ–¹å¼ï¼‰
            print(f"âš ï¸  MTCNNå‚æ•°å¼‚å¸¸: {str(e)[:100]}")
            print("ğŸ”„ å°è¯•æ— å‚æ•°åˆå§‹åŒ–MTCNNï¼ˆè¯Šæ–­æ¨èå®‰å…¨æ–¹æ¡ˆï¼‰...")
            try:
                self.detector = MTCNN()  # æ— å‚æ•°åˆå§‹åŒ–ï¼ˆè¯Šæ–­ç¡®è®¤å¯ç”¨ï¼‰
                print(f"âœ… MTCNNæ— å‚æ•°åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e2:
                # MTCNNå®Œå…¨ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°Haar
                print(f"âŒ MTCNNåˆå§‹åŒ–å¤±è´¥: {str(e2)[:100]}")
                self.method = 'haar'
                self._init_haar_detector()

    def _init_haar_detector(self):
        """åˆå§‹åŒ–Haar Cascadeå¤‡é€‰æ£€æµ‹å™¨ï¼ˆç¡®ä¿é™çº§å¯ç”¨ï¼‰"""
        # åŠ è½½OpenCVè‡ªå¸¦çš„äººè„¸çº§è”æ¨¡å‹ï¼ˆæ— éœ€é¢å¤–ä¸‹è½½ï¼‰
        cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        self.detector = cv2.CascadeClassifier(cascade_path)

        if self.detector.empty():
            raise RuntimeError(f"âŒ Haaræ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·¯å¾„: {cascade_path}")

        print(f"âœ… Haar Cascadeæ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"   æœ€å°äººè„¸å°ºå¯¸: {self.min_face_size}px")

    def detect(
            self,
            image: np.ndarray,
            return_landmarks: bool = True
    ) -> List[Dict]:
        """
        æ£€æµ‹äººè„¸ï¼ˆç»Ÿä¸€è¾“å‡ºæ ¼å¼ï¼Œå…¼å®¹MTCNN/Haarï¼‰

        Args:
            image: è¾“å…¥å›¾åƒï¼ˆBGRæ ¼å¼ï¼Œå¦‚cv2.imreadç»“æœï¼‰
            return_landmarks: æ˜¯å¦è¿”å›å…³é”®ç‚¹ï¼ˆä»…MTCNNç”Ÿæ•ˆï¼‰

        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœå«ï¼š
            - box: [x, y, width, height] äººè„¸æ¡†åæ ‡
            - confidence: ç½®ä¿¡åº¦ï¼ˆMTCNN: 0~1ï¼ŒHaar: 1.0ï¼‰
            - landmarks: å…³é”®ç‚¹å­—å…¸ï¼ˆä»…MTCNNï¼Œå«left_eye/right_eye/nose/mouth_left/mouth_rightï¼‰
        """
        # è¾“å…¥åˆæ³•æ€§æ£€æŸ¥
        if image is None or image.size == 0:
            return []

        # è®°å½•æ£€æµ‹è€—æ—¶
        start_time = time.time()

        # æŒ‰æ–¹æ³•æ‰§è¡Œæ£€æµ‹
        if self.method == 'mtcnn':
            results = self._detect_mtcnn(image, return_landmarks)
        else:
            results = self._detect_haar(image)

        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        elapsed = time.time() - start_time
        self.detection_count += 1
        self.total_time += elapsed

        return results

    def _detect_mtcnn(
            self,
            image: np.ndarray,
            return_landmarks: bool
    ) -> List[Dict]:
        """MTCNNæ£€æµ‹ï¼ˆé€‚é…è¯Šæ–­ç‰ˆè¾“å‡ºï¼Œå¢åŠ åè¿‡æ»¤é€»è¾‘ï¼‰"""
        # MTCNNè¦æ±‚è¾“å…¥RGBæ ¼å¼ï¼Œè½¬æ¢BGRâ†’RGB
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # æ‰§è¡Œæ£€æµ‹ï¼ˆè¯Šæ–­ç¡®è®¤detect_facesæ–¹æ³•å¯ç”¨ï¼‰
        raw_detections = self.detector.detect_faces(rgb_image)
        filtered_results = []

        # ç»“æœè¿‡æ»¤ï¼ˆå¼¥è¡¥MTCNNæ— åˆå§‹åŒ–å‚æ•°çš„é—®é¢˜ï¼‰
        for det in raw_detections:
            # 1. ç½®ä¿¡åº¦è¿‡æ»¤ï¼ˆæ’é™¤ä½ç½®ä¿¡ç»“æœï¼‰
            confidence = det.get('confidence', 0.0)
            if confidence < self.confidence_threshold:
                continue

            # 2. äººè„¸å°ºå¯¸è¿‡æ»¤ï¼ˆæ›¿ä»£MTCNNçš„min_face_sizeå‚æ•°ï¼‰
            box = det.get('box', [0, 0, 0, 0])  # [x, y, w, h]
            face_width, face_height = box[2], box[3]
            if face_width < self.min_face_size or face_height < self.min_face_size:
                continue

            # æ„å»ºç»Ÿä¸€è¾“å‡ºæ ¼å¼
            result = {
                'box': box,
                'confidence': round(confidence, 3)  # ä¿ç•™3ä½å°æ•°
            }

            # 3. å…³é”®ç‚¹å¤„ç†ï¼ˆä»…å½“éœ€è¦ä¸”å­˜åœ¨æ—¶æ·»åŠ ï¼‰
            if return_landmarks and 'keypoints' in det:
                raw_kps = det['keypoints']
                # æå–5ä¸ªæ ¸å¿ƒå…³é”®ç‚¹ï¼ˆä¸æ–‡æ¡£éœ€æ±‚ä¸€è‡´ï¼‰
                core_kps = {
                    'left_eye': raw_kps.get('left_eye', (0, 0)),
                    'right_eye': raw_kps.get('right_eye', (0, 0)),
                    'nose': raw_kps.get('nose', (0, 0)),
                    'mouth_left': raw_kps.get('mouth_left', (0, 0)),
                    'mouth_right': raw_kps.get('mouth_right', (0, 0))
                }
                result['landmarks'] = core_kps

            filtered_results.append(result)

        return filtered_results

    def _detect_haar(self, image: np.ndarray) -> List[Dict]:
        """Haar Cascadeæ£€æµ‹ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼Œç»Ÿä¸€è¾“å‡ºæ ¼å¼ï¼‰"""
        # Haarè¦æ±‚ç°åº¦å›¾è¾“å…¥
        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # æ‰§è¡Œæ£€æµ‹ï¼ˆç”¨min_face_sizeæ§åˆ¶æœ€å°å°ºå¯¸ï¼‰
        raw_faces = self.detector.detectMultiScale(
            gray_image,
            scaleFactor=1.1,  # Haarå›ºå®šå‚æ•°ï¼Œæå‡æ£€æµ‹é€Ÿåº¦
            minNeighbors=5,  # è¿‡æ»¤è¯¯æ£€
            minSize=(self.min_face_size, self.min_face_size)
        )

        # è½¬æ¢ä¸ºç»Ÿä¸€è¾“å‡ºæ ¼å¼ï¼ˆæ— å…³é”®ç‚¹ï¼Œç½®ä¿¡åº¦å›ºå®šä¸º1.0ï¼‰
        results = []
        for (x, y, w, h) in raw_faces:
            results.append({
                'box': [x, y, w, h],
                'confidence': 1.0,  # Haaræ— ç½®ä¿¡åº¦ï¼Œå›ºå®šä¸º1.0
                'landmarks': None  # Haarä¸æ”¯æŒå…³é”®ç‚¹
            })

        return results

    def draw_detections(
            self,
            image: np.ndarray,
            detections: List[Dict],
            draw_landmarks: bool = True
    ) -> np.ndarray:
        """
        åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœï¼ˆå¯è§†åŒ–éªŒè¯ï¼‰

        Args:
            image: åŸå§‹å›¾åƒï¼ˆBGRæ ¼å¼ï¼‰
            detections: detect()è¿”å›çš„æ£€æµ‹ç»“æœ
            draw_landmarks: æ˜¯å¦ç»˜åˆ¶å…³é”®ç‚¹ï¼ˆä»…MTCNNç»“æœç”Ÿæ•ˆï¼‰

        Returns:
            ç»˜åˆ¶åçš„å›¾åƒï¼ˆä¸ä¿®æ”¹åŸå›¾ï¼Œè¿”å›æ–°å›¾ï¼‰
        """
        output_image = image.copy()
        landmark_color = (255, 0, 0)  # å…³é”®ç‚¹é¢œè‰²ï¼šè“è‰²
        box_color = (0, 255, 0)  # äººè„¸æ¡†é¢œè‰²ï¼šç»¿è‰²
        text_color = (0, 255, 0)  # ç½®ä¿¡åº¦æ–‡å­—é¢œè‰²ï¼šç»¿è‰²

        for det in detections:
            x, y, w, h = det['box']
            confidence = det['confidence']

            # 1. ç»˜åˆ¶äººè„¸æ¡†
            cv2.rectangle(
                output_image,
                (x, y),  # å·¦ä¸Šè§’åæ ‡
                (x + w, y + h),  # å³ä¸‹è§’åæ ‡
                box_color,  # é¢œè‰²
                2  # çº¿å®½
            )

            # 2. ç»˜åˆ¶ç½®ä¿¡åº¦æ–‡å­—ï¼ˆä½äºæ¡†ä¸Šæ–¹ï¼‰
            text = f"Conf: {confidence:.2f}"
            cv2.putText(
                output_image,
                text,
                (x, max(0, y - 10)),  # æ–‡å­—ä½ç½®ï¼ˆé¿å…è¶…å‡ºå›¾åƒé¡¶éƒ¨ï¼‰
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,  # å­—ä½“å¤§å°
                text_color,
                2  # æ–‡å­—çº¿å®½
            )

            # 3. ç»˜åˆ¶å…³é”®ç‚¹ï¼ˆä»…MTCNNç»“æœï¼‰
            if draw_landmarks and det.get('landmarks'):
                landmarks = det['landmarks']
                for kp_name, (kp_x, kp_y) in landmarks.items():
                    # ç»˜åˆ¶å®å¿ƒåœ†å…³é”®ç‚¹ï¼ˆåŠå¾„3ï¼Œå¡«å……ï¼‰
                    cv2.circle(
                        output_image,
                        (kp_x, kp_y),
                        3,
                        landmark_color,
                        -1  # -1è¡¨ç¤ºå¡«å……åœ†
                    )

        return output_image

    def get_largest_face(self, detections: List[Dict]) -> Optional[Dict]:
        """è·å–æ£€æµ‹ç»“æœä¸­é¢ç§¯æœ€å¤§çš„äººè„¸ï¼ˆå•äººåœºæ™¯ä¸“ç”¨ï¼‰"""
        if not detections:
            return None
        # æŒ‰äººè„¸é¢ç§¯ï¼ˆå®½Ã—é«˜ï¼‰æ’åºï¼Œå–æœ€å¤§
        return max(detections, key=lambda det: det['box'][2] * det['box'][3])

    def get_performance_stats(self) -> Dict:
        """è·å–æ£€æµ‹æ€§èƒ½ç»Ÿè®¡ï¼ˆç¬¦åˆæ–‡æ¡£æ€§èƒ½ç›‘æ§éœ€æ±‚ï¼‰"""
        if self.detection_count == 0:
            return {
                'total_detections': 0,
                'avg_time_ms': 0.0,
                'avg_fps': 0.0,
                'total_time_s': 0.0
            }

        avg_time_ms = (self.total_time / self.detection_count) * 1000  # å¹³å‡è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
        avg_fps = self.detection_count / self.total_time  # å¹³å‡å¸§ç‡ï¼ˆFPSï¼‰

        return {
            'total_detections': self.detection_count,
            'avg_time_ms': round(avg_time_ms, 2),
            'avg_fps': round(avg_fps, 2),
            'total_time_s': round(self.total_time, 2),
            'meets_target': avg_time_ms <= 50  # æ˜¯å¦æ»¡è¶³å•å¸§â‰¤50msç›®æ ‡
        }

    def reset_performance_stats(self):
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡ï¼ˆç”¨äºå¤šè½®æµ‹è¯•ï¼‰"""
        self.detection_count = 0
        self.total_time = 0.0


# ===================== è¾…åŠ©å·¥å…·å‡½æ•°ï¼ˆäººè„¸å¯¹é½ï¼Œå¯é€‰æ‰©å±•ï¼‰ =====================
def align_face(
        image: np.ndarray,
        landmarks: Dict,
        output_size: Tuple[int, int] = (224, 224)
) -> Optional[np.ndarray]:
    """
    æ ¹æ®å…³é”®ç‚¹å¯¹é½äººè„¸ï¼ˆç”¨äºåç»­ROIæå–ï¼Œç¬¦åˆæ–‡æ¡£ä¸‹ä¸€æ­¥éœ€æ±‚ï¼‰

    Args:
        image: åŸå§‹å›¾åƒï¼ˆBGRæ ¼å¼ï¼‰
        landmarks: MTCNNè¿”å›çš„å…³é”®ç‚¹å­—å…¸
        output_size: å¯¹é½åäººè„¸å°ºå¯¸

    Returns:
        å¯¹é½åçš„äººè„¸å›¾åƒï¼ˆRGBæ ¼å¼ï¼‰ï¼Œå¤±è´¥è¿”å›None
    """
    # éªŒè¯å…³é”®ç‚¹å®Œæ•´æ€§
    required_kps = ['left_eye', 'right_eye', 'nose']
    if not all(kp in landmarks for kp in required_kps):
        print("âš ï¸  å…³é”®ç‚¹ä¸å®Œæ•´ï¼Œæ— æ³•å¯¹é½äººè„¸")
        return None

    left_eye = np.array(landmarks['left_eye'], dtype=np.float32)
    right_eye = np.array(landmarks['right_eye'], dtype=np.float32)
    nose = np.array(landmarks['nose'], dtype=np.float32)

    # 1. è®¡ç®—åŒçœ¼ä¸­å¿ƒç‚¹å’Œæ—‹è½¬è§’åº¦ï¼ˆçº æ­£äººè„¸å€¾æ–œï¼‰
    eye_center = (left_eye + right_eye) / 2  # åŒçœ¼ä¸­å¿ƒç‚¹
    eye_angle = np.degrees(np.arctan2(right_eye[1] - left_eye[1], right_eye[0] - left_eye[0]))

    # 2. æ„å»ºæ—‹è½¬çŸ©é˜µï¼ˆä»¥åŒçœ¼ä¸­å¿ƒä¸ºæ—‹è½¬ç‚¹ï¼Œçº æ­£è§’åº¦ï¼‰
    rotation_matrix = cv2.getRotationMatrix2D(
        center=(int(eye_center[0]), int(eye_center[1])),
        angle=eye_angle,
        scale=1.0
    )

    # 3. æ—‹è½¬å›¾åƒï¼ˆçº æ­£äººè„¸å€¾æ–œï¼‰
    h, w = image.shape[:2]
    aligned_image = cv2.warpAffine(
        image,
        rotation_matrix,
        (w, h),
        flags=cv2.INTER_CUBIC  # é«˜è´¨é‡æ’å€¼
    )

    # 4. è£å‰ªäººè„¸åŒºåŸŸï¼ˆåŸºäºé¼»å­å’ŒåŒçœ¼è·ç¦»ï¼‰
    eye_distance = np.linalg.norm(right_eye - left_eye)  # åŒçœ¼é—´è·
    face_width = int(eye_distance * 2.5)  # äººè„¸å®½åº¦ï¼ˆåŒçœ¼é—´è·çš„2.5å€ï¼‰
    face_height = int(face_width * 1.3)  # äººè„¸é«˜åº¦ï¼ˆå®½é«˜æ¯”1:1.3ï¼‰

    # è®¡ç®—è£å‰ªåæ ‡ï¼ˆä»¥é¼»å­ä¸ºä¸­å¿ƒï¼‰
    x1 = max(0, int(nose[0] - face_width / 2))
    y1 = max(0, int(nose[1] - face_height / 2))
    x2 = min(w, x1 + face_width)
    y2 = min(h, y1 + face_height)

    # è£å‰ªå¹¶ç¼©æ”¾è‡³ç›®æ ‡å°ºå¯¸
    face_crop = aligned_image[y1:y2, x1:x2]
    if face_crop.size == 0:
        return None

    # è½¬æ¢ä¸ºRGBæ ¼å¼ï¼ˆé€‚é…åç»­æ¨¡å‹è¾“å…¥ï¼‰
    face_rgb = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
    return cv2.resize(face_rgb, output_size, interpolation=cv2.INTER_CUBIC)


# ===================== æµ‹è¯•ä»£ç ï¼ˆæœ¬åœ°éªŒè¯ç”¨ï¼Œä¸æ–‡æ¡£æ­¥éª¤3ä¸€è‡´ï¼‰ =====================
def test_face_detector_full(
        test_video_path: str = "../../test_videos/test_video_1.avi",  # é€‚é…æœ¬åœ°è·¯å¾„
        test_frame_count: int = 10  # æµ‹è¯•å¸§æ•°ï¼ˆæ–‡æ¡£æå–10å¸§ï¼‰
):
    """
    å®Œæ•´æµ‹è¯•å‡½æ•°ï¼ˆä¸æ–‡æ¡£æ­¥éª¤3åŸºç¡€åŠŸèƒ½æµ‹è¯•ä¸€è‡´ï¼‰

    Args:
        test_video_path: æµ‹è¯•è§†é¢‘è·¯å¾„ï¼ˆæœ¬åœ°ç›¸å¯¹è·¯å¾„ï¼‰
        test_frame_count: æå–çš„æµ‹è¯•å¸§æ•°
    """
    print("=" * 70)
    print("ğŸ“ äººè„¸æ£€æµ‹å™¨å®Œæ•´æµ‹è¯•ï¼ˆé€‚é…MTCNNè¯Šæ–­ç‰ˆï¼‰")
    print("=" * 70)

    # 1. åˆå§‹åŒ–æ£€æµ‹å™¨ï¼ˆä¼˜å…ˆMTCNNï¼‰
    print("\nã€1/4ã€‘åˆå§‹åŒ–æ£€æµ‹å™¨")
    try:
        detector = FaceDetector(
            method='mtcnn',
            min_face_size=40,
            confidence_threshold=0.9
        )
        print(f"âœ… æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆï¼Œå½“å‰æ–¹æ³•: {detector.method}")
    except Exception as e:
        print(f"âŒ æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return

    # 2. æå–æµ‹è¯•è§†é¢‘å¸§ï¼ˆæ–‡æ¡£æ­¥éª¤3.1æå–10å¸§ï¼‰
    print(f"\nã€2/4ã€‘æå–æµ‹è¯•è§†é¢‘å¸§ï¼ˆå…±{test_frame_count}å¸§ï¼‰")
    if not os.path.exists(test_video_path):
        print(f"âš ï¸  æµ‹è¯•è§†é¢‘ä¸å­˜åœ¨: {test_video_path}")
        print("ğŸ’¡ è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º'test_videos'ï¼Œæ”¾å…¥test_video_1.avi")
        return

    # è¯»å–è§†é¢‘å¹¶æå–å¸§
    cap = cv2.VideoCapture(test_video_path)
    test_frames = []
    frame_idx = 0
    while cap.isOpened() and frame_idx < test_frame_count:
        ret, frame = cap.read()
        if ret:
            test_frames.append(frame)
            frame_idx += 1
        else:
            break
    cap.release()

    if not test_frames:
        print(f"âŒ æ— æ³•æå–è§†é¢‘å¸§ï¼ˆè§†é¢‘æŸåæˆ–æ ¼å¼ä¸æ”¯æŒï¼‰")
        return
    print(f"âœ… æˆåŠŸæå– {len(test_frames)} å¸§æµ‹è¯•æ•°æ®")

    # 3. æ‰§è¡ŒåŸºç¡€æ£€æµ‹ï¼ˆæ–‡æ¡£æ­¥éª¤3.1æ€§èƒ½æµ‹è¯•ï¼‰
    print(f"\nã€3/4ã€‘æ‰§è¡Œäººè„¸æ£€æµ‹ï¼ˆæ€§èƒ½ç»Ÿè®¡ï¼‰")
    detector.reset_performance_stats()  # é‡ç½®ç»Ÿè®¡
    test_image_count = 3  # æ–‡æ¡£æµ‹è¯•3å¼ å›¾
    test_iterations = 3  # æ–‡æ¡£æ¯å¼ é‡å¤3æ¬¡

    for img_idx in range(min(test_image_count, len(test_frames))):
        frame = test_frames[img_idx]
        print(f"\nğŸ“· æµ‹è¯•å›¾ç‰‡ {img_idx + 1}/{test_image_count}")

        for iter_idx in range(test_iterations):
            detections = detector.detect(frame)
            # è·å–å•æ¬¡æ£€æµ‹è€—æ—¶ï¼ˆæ€»è€—æ—¶å·®ï¼‰
            stats = detector.get_performance_stats()
            single_time_ms = stats['avg_time_ms'] if stats['total_detections'] > 0 else 0.0

            print(f"   è¿­ä»£ {iter_idx + 1}/{test_iterations}: "
                  f"äººè„¸æ•°={len(detections)}, "
                  f"è€—æ—¶={single_time_ms:.2f}ms")

    # 4. è¾“å‡ºæ€§èƒ½ç»Ÿè®¡ï¼ˆæ–‡æ¡£æ­¥éª¤3.1é¢„æœŸè¾“å‡ºï¼‰
    print(f"\nã€4/4ã€‘æ€§èƒ½ç»Ÿè®¡æ±‡æ€»ï¼ˆç›®æ ‡ï¼šå•å¸§â‰¤50msï¼‰")
    final_stats = detector.get_performance_stats()
    print(f"ğŸ“ˆ æ€§èƒ½ç»“æœ:")
    print(f"   æ€»æ£€æµ‹æ¬¡æ•°: {final_stats['total_detections']}")
    print(f"   å¹³å‡è€—æ—¶: {final_stats['avg_time_ms']} ms/å¸§")
    print(f"   å¹³å‡å¸§ç‡: {final_stats['avg_fps']} FPS")
    print(f"   æ€§èƒ½è¾¾æ ‡: {'âœ…' if final_stats['meets_target'] else 'âŒ'}")

    # 5. ä¿å­˜æ£€æµ‹ç»“æœå›¾ç‰‡ï¼ˆæ–‡æ¡£æ­¥éª¤3.2ï¼‰
    if test_frames:
        first_frame = test_frames[0]
        first_detections = detector.detect(first_frame)
        if first_detections:
            # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆæ–‡æ¡£è·¯å¾„ï¼‰
            output_dir = "../../test_output/detection"
            os.makedirs(output_dir, exist_ok=True)
            output_path = f"{output_dir}/face_detection_demo.jpg"

            # ç»˜åˆ¶å¹¶ä¿å­˜ç»“æœ
            result_img = detector.draw_detections(first_frame, first_detections)
            cv2.imwrite(output_path, result_img)
            print(f"\nâœ… æ£€æµ‹ç»“æœå·²ä¿å­˜: {output_path}")

            # æ‰“å°æ£€æµ‹è¯¦æƒ…ï¼ˆæ–‡æ¡£æ­¥éª¤3.1é¢„æœŸè¾“å‡ºï¼‰
            print(f"\nğŸ“‹ ç¬¬ä¸€å¸§æ£€æµ‹è¯¦æƒ…:")
            largest_face = detector.get_largest_face(first_detections)
            if largest_face:
                print(f"   æœ€å¤§äººè„¸ä½ç½®: {largest_face['box']}")
                print(f"   ç½®ä¿¡åº¦: {largest_face['confidence']}")
                if largest_face.get('landmarks'):
                    print(f"   æ ¸å¿ƒå…³é”®ç‚¹:")
                    for kp_name, (x, y) in largest_face['landmarks'].items():
                        print(f"     {kp_name}: ({x}, {y})")

    print("\n" + "=" * 70)
    print("âœ… æµ‹è¯•å®Œæˆï¼ˆç¬¦åˆæ–‡æ¡£æ­¥éª¤3åŸºç¡€åŠŸèƒ½æµ‹è¯•è¦æ±‚ï¼‰")
    print("=" * 70)


# æœ¬åœ°è¿è¡Œæµ‹è¯•ï¼ˆç›´æ¥æ‰§è¡Œè„šæœ¬æ—¶è§¦å‘ï¼‰
if __name__ == "__main__":
    # æ‰§è¡Œå®Œæ•´æµ‹è¯•ï¼ˆä¸æ–‡æ¡£æ­¥éª¤3ä¸€è‡´ï¼‰
    test_face_detector_full(
        test_video_path="../../test_videos/test_video_1.avi",  # æœ¬åœ°è§†é¢‘è·¯å¾„
        test_frame_count=10  # æå–10å¸§æµ‹è¯•ï¼ˆæ–‡æ¡£è¦æ±‚ï¼‰
    )
